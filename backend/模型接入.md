import pandas as pd
import numpy as np
import joblib
import os

# --- 尝试导入模型库 ---
try:
    import xgboost as xgb
except ImportError:
    print("错误: XGBoost库未安装，如果预测流程中包含XGBoost，请安装它。")
    xgb = None
try:
    import lightgbm as lgb
except ImportError:
    print("错误: LightGBM库未安装，如果预测流程中包含LightGBM，请安装它。")
    lgb = None
try:
    from pytorch_tabnet.tab_model import TabNetClassifier
    import torch
except ImportError:
    print("错误: pytorch-tabnet或PyTorch未安装，如果预测流程中包含TabNet，请安装它。")
    TabNetClassifier = None
    torch = None

# --- 定义加载路径 ---
MODEL_DIR = "saved_stroke_model" # 包含所有最终模型和preprocessing_bundle.joblib的目录

def load_preprocessing_artifacts(model_dir: str):
    """加载所有预处理相关的对象和信息。"""
    bundle_path = os.path.join(model_dir, "preprocessing_bundle.joblib")
    if not os.path.exists(bundle_path):
        print(f"错误: 预处理信息包 'preprocessing_bundle.joblib' 在 '{model_dir}' 未找到！")
        return None
    try:
        preprocessing_info = joblib.load(bundle_path)
        print("预处理信息包已成功加载。")
        # 从bundle中提取各个组件
        scaler = preprocessing_info.get('scaler')
        final_feature_columns = preprocessing_info.get('final_feature_columns')
        all_categorical_cols_to_encode = preprocessing_info.get('all_categorical_cols_to_encode')
        numerical_cols_to_scale = preprocessing_info.get('numerical_cols_to_scale')
        ds1_symptom_cols = preprocessing_info.get('ds1_symptom_cols')
        filling_values = preprocessing_info.get('filling_values', {}) # 如果没有保存，则为空字典

        if not all([scaler, final_feature_columns, all_categorical_cols_to_encode, numerical_cols_to_scale, ds1_symptom_cols]):
            print("警告: 预处理信息包中缺少一个或多个关键组件！")
            # 你可以根据需要决定是否在这里返回None或抛出错误

        return scaler, final_feature_columns, all_categorical_cols_to_encode, numerical_cols_to_scale, ds1_symptom_cols, filling_values
    except Exception as e:
        print(f"加载 preprocessing_bundle.joblib 时发生错误: {e}")
        return None


def load_models(model_dir: str, l0_model_names_order: list):
    """加载所有Level 0模型和元学习器。"""
    loaded_l0_models = {}
    meta_model_loaded = None
    try:
        # 加载Level 0模型
        for model_name in l0_model_names_order:
            if model_name == "XGBoost" and xgb:
                model_path = os.path.join(model_dir, "final_xgb_model.json")
                if os.path.exists(model_path):
                    xgb_model = xgb.XGBClassifier()
                    xgb_model.load_model(model_path)
                    loaded_l0_models["XGBoost"] = xgb_model
                    print("XGBoost模型已加载。")
                else: print(f"警告: XGBoost模型文件 {model_path} 未找到。")
            elif model_name == "LightGBM" and lgb:
                model_path = os.path.join(model_dir, "final_lgb_model.joblib")
                if os.path.exists(model_path):
                    loaded_l0_models["LightGBM"] = joblib.load(model_path)
                    print("LightGBM模型已加载。")
                else: print(f"警告: LightGBM模型文件 {model_path} 未找到。")
            elif model_name == "TabNet" and TabNetClassifier and torch:
                model_path = os.path.join(model_dir, "final_tabnet_model.zip") # TabNet保存为zip
                if os.path.exists(model_path):
                    tab_model = TabNetClassifier()
                    tab_model.load_model(model_path)
                    loaded_l0_models["TabNet"] = tab_model
                    print("TabNet模型已加载。")
                else: print(f"警告: TabNet模型文件 {model_path} 未找到。")
        
        # 加载元学习器
        meta_model_path = os.path.join(model_dir, "final_meta_learner.joblib")
        if os.path.exists(meta_model_path):
            meta_model_loaded = joblib.load(meta_model_path)
            print("元学习器已加载。")
        else:
            print(f"错误: 元学习器文件 {meta_model_path} 未找到！")
            return None, None # 元学习器是必需的

        # 检查是否所有在l0_model_names_order中的模型都被成功加载了
        if len(loaded_l0_models) != len(l0_model_names_order):
            print("警告: 并非所有在l0_model_names_order中指定的Level 0模型都被成功加载。")
            print(f"期望模型: {l0_model_names_order}")
            print(f"实际加载的模型: {list(loaded_l0_models.keys())}")
            # 你可能需要决定如何处理这种情况，是报错还是继续（如果元学习器能处理部分缺失的输入？）
            # 通常应该报错，因为元学习器的输入维度是固定的。

        return loaded_l0_models, meta_model_loaded

    except Exception as e:
        print(f"加载模型时发生错误: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def preprocess_new_data(new_data_df: pd.DataFrame,
                        scaler: object,
                        all_categorical_cols_to_encode: list,
                        numerical_cols_to_scale: list,
                        ds1_symptom_cols: list,
                        final_feature_columns: list,
                        filling_values: dict
                        ) -> pd.DataFrame:
    """
    对新的输入数据应用与训练时完全相同的预处理步骤。
    【【【 这是此脚本中最需要你根据Phase 1代码仔细定制的部分 】】】
    """
    print("--- 开始预处理新数据 ---")
    processed_df = new_data_df.copy()

    # 1. 标准化列名 (与Phase 1一致)
    processed_df.columns = processed_df.columns.str.lower().str.replace(' ', '_').str.replace('&', 'and').str.replace('/', '_').str.replace('(', '').str.replace(')', '')
    print("  步骤1: 列名已标准化。")

    # 2. 确保所有期望的原始列都存在，并处理缺失值
    # 构建一个更全面的期望原始列列表 (这些是独热编码和缩放之前的列)
    # 最好在Phase 1保存一个包含所有原始输入特征（合并后，特征工程前）的列表
    expected_raw_cols_set = set(all_categorical_cols_to_encode + numerical_cols_to_scale + ds1_symptom_cols +
                                ['age','gender', 'hypertension', 'heart_disease', 'ever_married',
                                 'work_type', 'residence_type', 'avg_glucose_level', 'bmi',
                                 'smoking_status','stroke_risk_percentage_ds1']) # 确保所有可能的原始列都在这里

    for col in expected_raw_cols_set:
        if col not in processed_df.columns:
            print(f"  警告: 新数据中缺少原始列 '{col}'. 将根据策略填充。")
            if col in ds1_symptom_cols or col == 'hypertension' or col == 'heart_disease':
                processed_df[col] = filling_values.get(f'default_{col}', 0) # 示例
            elif col == 'stroke_risk_percentage_ds1':
                processed_df[col] = filling_values.get('ds1_stroke_risk_mean_fill', 55.0)
            elif col == 'avg_glucose_level':
                 processed_df[col] = filling_values.get('ds2_avg_glucose_median_for_ds1_rows', 90.0)
            elif col == 'bmi':
                 processed_df[col] = filling_values.get('ds2_bmi_median_for_ds1_rows', 28.0)
            elif col in all_categorical_cols_to_encode:
                processed_df[col] = filling_values.get(f'default_cat_{col}', 'Unknown')
            else: # 其他数值列
                processed_df[col] = filling_values.get(f'default_num_{col}', 0.0)
    print("  步骤2: 期望列检查与初步缺失处理完成。")

    # 3. 特定列的类型转换和映射 (与Phase 1一致)
    if 'ever_married' in processed_df.columns and processed_df['ever_married'].dtype == 'object':
        processed_df['ever_married'] = processed_df['ever_married'].map({'Yes': 1, 'No': 0}).fillna(filling_values.get('default_ever_married_fill', 0)).astype(int)
    
    for col_name in ['hypertension', 'heart_disease', 'ever_married']: # ever_married可能已经是int
         if col_name in processed_df.columns:
            processed_df[col_name] = pd.to_numeric(processed_df[col_name], errors='coerce').fillna(filling_values.get(f'default_{col_name}', 0)).astype(int)

    for col_name in ds1_symptom_cols:
        if col_name in processed_df.columns:
            processed_df[col_name] = pd.to_numeric(processed_df[col_name], errors='coerce').fillna(filling_values.get(f'default_{col_name}', 0)).astype(int)
    
    for col_name in ['smoking_status', 'gender', 'work_type', 'residence_type'] + [col for col in all_categorical_cols_to_encode if 'group' in col or 'category' in col]: # 包括分箱特征
        if col_name in processed_df.columns and processed_df[col_name].isnull().any():
            processed_df[col_name] = processed_df[col_name].fillna(filling_values.get(f'default_cat_{col_name}', 'Unknown'))
    print("  步骤3: 特定列类型转换和映射完成。")

    # 4. 特征工程 (必须与Phase 1完全一致！)
    print("  步骤4: 应用特征工程...")
    symptom_cols_present = [s_col for s_col in ds1_symptom_cols if s_col in processed_df.columns]
    processed_df['num_symptoms_ds1'] = processed_df[symptom_cols_present].sum(axis=1) if symptom_cols_present else 0

    processed_df['bmi_x_age'] = processed_df['bmi'].astype(float) * processed_df['age'].astype(float) if 'bmi' in processed_df.columns and 'age' in processed_df.columns else 0.0
    processed_df['glucose_x_hypertension'] = processed_df['avg_glucose_level'].astype(float) * processed_df['hypertension'].astype(float) if 'avg_glucose_level' in processed_df.columns and 'hypertension' in processed_df.columns else 0.0

    age_bins = [0, 18, 35, 50, 65, float('inf')]; age_labels = ['0-18', '19-35', '36-50', '51-65', '65+']
    if 'age' in processed_df.columns:
        processed_df['age_group'] = pd.cut(processed_df['age'], bins=age_bins, labels=age_labels, right=True)
        if 'Unknown' not in processed_df['age_group'].cat.categories: processed_df['age_group'] = processed_df['age_group'].cat.add_categories('Unknown')
        processed_df['age_group'] = processed_df['age_group'].fillna('Unknown')
    else: processed_df['age_group'] = 'Unknown' # 如果age列缺失

    # ... (类似地为 bmi_category, glucose_category 添加更鲁棒的创建和填充) ...
    if 'bmi' in processed_df.columns:
        bmi_bins = [0, 18.5, 24.9, 29.9, 34.9, 39.9, float('inf')]; bmi_labels = ['Underweight', 'Normal', 'Overweight', 'Obese_Class1', 'Obese_Class2', 'Obese_Class3']
        processed_df['bmi_category'] = pd.cut(processed_df['bmi'], bins=bmi_bins, labels=bmi_labels, right=False)
        if 'Unknown' not in processed_df['bmi_category'].cat.categories: processed_df['bmi_category'] = processed_df['bmi_category'].cat.add_categories('Unknown')
        processed_df['bmi_category'] = processed_df['bmi_category'].fillna('Unknown')
    else: processed_df['bmi_category'] = 'Unknown'

    if 'avg_glucose_level' in processed_df.columns:
        glucose_bins = [0, 100, 126, float('inf')]; glucose_labels = ['Normal_Glucose', 'Prediabetes_Glucose', 'Diabetes_Glucose']
        processed_df['glucose_category'] = pd.cut(processed_df['avg_glucose_level'], bins=glucose_bins, labels=glucose_labels, right=False)
        if 'Unknown' not in processed_df['glucose_category'].cat.categories: processed_df['glucose_category'] = processed_df['glucose_category'].cat.add_categories('Unknown')
        processed_df['glucose_category'] = processed_df['glucose_category'].fillna('Unknown')
    else: processed_df['glucose_category'] = 'Unknown'


    if 'smoking_status' in processed_df.columns:
        processed_df['is_smoker'] = processed_df['smoking_status'].apply(lambda x: 1 if str(x) in ['smokes', 'formerly smoked'] else 0)
        processed_df['smoking_and_hypertension'] = processed_df['is_smoker'] * processed_df['hypertension'].astype(int) if 'hypertension' in processed_df.columns else 0
        if 'bmi' in processed_df.columns:
            is_bmi_high_series = processed_df['bmi'].apply(lambda x: 1 if pd.to_numeric(x, errors='coerce') >= 25 else 0)
            processed_df['bmi_high_and_smoking'] = is_bmi_high_series * processed_df['is_smoker']
        else: processed_df['bmi_high_and_smoking'] = 0
    else: processed_df['is_smoker'] = 0; processed_df['smoking_and_hypertension'] = 0; processed_df['bmi_high_and_smoking'] = 0
    print("  特征工程应用完毕。")

    # 5. 独热编码
    actual_cols_to_dummify = [col for col in all_categorical_cols_to_encode if col in processed_df.columns]
    if actual_cols_to_dummify:
        print(f"  步骤5: 将对以下列进行独热编码: {actual_cols_to_dummify}")
        processed_df = pd.get_dummies(processed_df, columns=actual_cols_to_dummify, dummy_na=False, prefix=actual_cols_to_dummify, prefix_sep='_')
    else: print("  警告: 没有在当前数据中找到需要独热编码的列。")
    print("  独热编码完成。")

    # 6. 对齐特征列
    print("  步骤6: 对齐特征列...")
    if not final_feature_columns: raise ValueError("错误: final_feature_columns 列表为空。")
    current_cols = set(processed_df.columns)
    expected_cols = set(final_feature_columns)
    missing_cols = list(expected_cols - current_cols)
    if missing_cols:
        for c in missing_cols: processed_df[c] = 0
    extra_cols = list(current_cols - expected_cols)
    if extra_cols:
        processed_df = processed_df.drop(columns=extra_cols)
    processed_df = processed_df[final_feature_columns]
    print("  特征列已对齐。")

    # 7. 缩放数值特征
    cols_to_scale_in_final_df = [col for col in numerical_cols_to_scale if col in processed_df.columns]
    if cols_to_scale_in_final_df and scaler:
        print(f"  步骤7: 将对以下数值列进行缩放: {cols_to_scale_in_final_df[:5]}...")
        processed_df[cols_to_scale_in_final_df] = scaler.transform(processed_df[cols_to_scale_in_final_df])
        print("  数值特征已缩放。")
    # ... (错误处理如前) ...
    
    # 8. 转换为 float32
    print("  步骤8: 转换为float32...")
    try:
        processed_df = processed_df.astype('float32')
    except ValueError as e:
        print(f"astype('float32') 转换失败: {e}")
        for col in processed_df.columns:
            if not pd.api.types.is_numeric_dtype(processed_df[col]):
                print(f"错误转换的列 '{col}' 类型: {processed_df[col].dtype}, 值 (前5): {processed_df[col].unique()[:5]}")
        raise
    print("--- 数据预处理完成 ---")
    return processed_df


def predict_stroke_risk(new_patient_data_df: pd.DataFrame):
    """
    加载保存的模型和预处理对象，对新患者数据进行预测。
    """
    # 1. 加载预处理组件
    loaded_artifacts = load_preprocessing_artifacts(MODEL_DIR)
    if loaded_artifacts is None: return None
    scaler, final_feature_columns, all_categorical_cols_to_encode, \
    numerical_cols_to_scale, ds1_symptom_cols, filling_values = loaded_artifacts
    
    # 2. 加载 Level 0 模型名称顺序
    l0_order_path = os.path.join(MODEL_DIR, "l0_model_names_order.joblib")
    if not os.path.exists(l0_order_path):
        print(f"错误: Level 0 模型顺序文件 'l0_model_names_order.joblib' 在 '{MODEL_DIR}' 未找到！")
        return None
    try:
        l0_model_names_order = joblib.load(l0_order_path)
        print(f"Level 0 模型顺序已加载: {l0_model_names_order}")
    except Exception as e:
        print(f"加载 l0_model_names_order.joblib 时发生错误: {e}"); return None
        
    # 3. 加载模型
    loaded_l0_models, meta_model_loaded = load_models(MODEL_DIR, l0_model_names_order)
    if loaded_l0_models is None or meta_model_loaded is None:
        print("一个或多个模型未能加载，中止预测。"); return None
    if len(loaded_l0_models) != len(l0_model_names_order):
        print("警告: 加载的Level 0模型数量与期望的顺序列表数量不符。"); # 可能需要更强的错误处理

    # 4. 预处理新数据
    print("\n对新数据进行预处理...")
    try:
        X_processed_new = preprocess_new_data(
            new_patient_data_df, scaler, all_categorical_cols_to_encode,
            numerical_cols_to_scale, ds1_symptom_cols, final_feature_columns,
            filling_values # 传递加载的填充值字典
        )
        if X_processed_new.empty: print("预处理返回空DataFrame，中止。"); return None
        print(f"预处理后新数据形状: {X_processed_new.shape}")
        if X_processed_new.shape[1] != len(final_feature_columns):
            print(f"错误: 预处理后特征数量与训练时 ({len(final_feature_columns)}) 不符！"); return None
    except Exception as e:
        print(f"错误：预处理新数据时失败: {e}"); import traceback; traceback.print_exc(); return None

    # 5. 获取Level 0模型的预测
    print("\n获取Level 0模型的预测...")
    l0_predictions_new_dict = {}
    for model_name in l0_model_names_order:
        if model_name in loaded_l0_models:
            model = loaded_l0_models[model_name]
            input_data_for_model = X_processed_new.values if model_name == "TabNet" else X_processed_new
            l0_predictions_new_dict[model_name] = model.predict_proba(input_data_for_model)[:, 1]
            print(f"  {model_name} 预测完成。")
        else: print(f"错误: 模型 '{model_name}' 未加载!"); return None
    meta_features_new_list = [l0_predictions_new_dict[model_name] for model_name in l0_model_names_order]
    meta_features_new = np.column_stack(meta_features_new_list)
    print(f"新数据元特征形状: {meta_features_new.shape}")

    # 6. 进行最终预测
    print("\n进行最终预测...")
    final_risk_proba = meta_model_loaded.predict_proba(meta_features_new)[:, 1]
    return final_risk_proba


if __name__ == '__main__':
    print("\n--- 开始预测示例 ---")
    # --- 创建与你Phase 1原始输入结构完全一致的示例数据 ---
    # 【【【 列名和数据类型必须与你喂给Phase 1脚本的CSV文件中的完全一致 】】】
    sample_data_raw = {
        'age': [119.0, 45.0, 30.0],
        'gender': ['Male', 'Female', 'Male'],
        'hypertension': [0, 1, 0], # 0 或 1
        'heart_disease': [0, 0, 0], # 0 或 1
        'ever_married': ['Yes', 'No', 'Yes'], # 'Yes' 或 'No'
        'work_type': ['Private', 'Self-employed', 'Govt_job'],
        'residence_type': ['Urban', 'Rural', 'Urban'], # 注意Phase 1列名标准化是 Residence_type -> residence_type
        'avg_glucose_level': [228.69, 90.0, 150.0],
        'bmi': [36.6, 25.0, 22.0],
        'smoking_status': ['formerly smoked', 'never smoked', 'smokes'],
        # DS1 症状列 (0 或 1)
        'chest_pain': [1,0,1], 'shortness_of_breath': [1,1,0], 'irregular_heartbeat': [1,0,0],
        'fatigue_&_weakness': [1,0,1], 'dizziness': [1,0,0], 'swelling_(edema)': [1,1,0], # 注意原始列名中的特殊字符
        'pain_in_neck/jaw/shoulder/back': [0,0,0], 'excessive_sweating': [1,0,1], 'persistent_cough': [1,0,0],
        'nausea/vomiting': [1,0,0], 'high_blood_pressure': [1,1,0], # 这是DS1的
        'chest_discomfort_(activity)': [0,0,0], 'cold_hands/feet': [0,0,0],
        'snoring/sleep_apnea': [0,1,1], 'anxiety/feeling_of_doom': [1,0,0],
        # DS1 的 stroke_risk_%
        'stroke_risk_(%)': [np.nan, 50.0, 75.0] # 列名与原始CSV一致
    }
    # 确保示例数据包含所有模型期望的原始列
    # 最好动态加载ds1_symptom_cols来确保所有症状列都包括
    # 但由于preprocess_new_data开头有检查和填充，这里可以简化
    
    new_patients_df = pd.DataFrame(sample_data_raw)
    print("\n示例新患者数据 (原始格式，列名应与CSV文件一致):")
    print(new_patients_df.to_string())

    predicted_risks = predict_stroke_risk(new_patients_df)

    if predicted_risks is not None:
        print("\n预测的脑卒中风险概率:")
        for i, risk in enumerate(predicted_risks):
            print(f"  患者 {i+1}: {risk:.4f} ({(risk*100):.2f}%)")